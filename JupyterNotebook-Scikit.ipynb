{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    " NEGATIVE = \"Negative\"\n",
    " NEUTRAL = \"Neutral\"\n",
    " POSITIVE = \"Positive-Joh\"\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "\n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else:\n",
    "            return Sentiment.POSITIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Love the book, great story line, keeps you entertained.for a first novel from this author she did a great job,  Would definitely recommend!'"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = 'Books_small.json'\n",
    "reviews = []\n",
    "\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        #print (line)\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'], review['overall'])) #very nice way for capsuling raw data into class object\n",
    "\n",
    "reviews[5].text\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting into train-test\n",
    "training, test =train_test_split(reviews, test_size=0.33, random_state=42 )\n",
    "\n",
    "train_x = [x.text for x in training]\n",
    "train_y = [y.sentiment for y in training]\n",
    "\n",
    "test_x = [x.text for x in test]\n",
    "test_y = [y.sentiment for y in test]\n",
    "\n",
    "test_y[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_x) # the fit part in fit_transform involves learning the parameters mean and variance to center the data (z-scoring). The parameters will be automatically stored internally. Then the transform part is called to get the list of words into the word-occurence matrix. When testing the model it is only necessary to call transform part as the internally parameters will then be used. This is important because for test data you must not recalculate mean and variance: This would be a sort of learning process and belongs to training otherwise there is overfitting possible. Also the test data is only too small and mean and variance would not be representative for all test samples in the world.\n",
    "# Basic Principle in Machine Learning: Learned parameters/hyperparameters in training have to fixed for testing\n",
    "\n",
    "test_x_vectors = vectorizer.transform(test_x) #fitted parameters will be taken from above (internal storage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and basic evaluation (no x-valid)\n",
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.91319444])"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# basic two steps for all classifiers: Call,specify classifier and fit\n",
    "clf_svm = svm.SVC(kernel = 'linear') #Call and specify \n",
    "\n",
    "clf_svm.fit(train_x_vectors, train_y) #fit\n",
    "\n",
    "# just testing\n",
    "test_x[0] \n",
    "clf_svm.predict(test_x_vectors[0]) \n",
    "\n",
    "#evaluation (only one fold)\n",
    "clf_svm.score(test_x_vectors, test_y) # Accuracy\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels=[Sentiment.POSITIVE]) # F1 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitpyenvvenv97a7fb0947ed4ee08f31e6927c206f4d",
   "display_name": "Python 3.7.3 64-bit ('py-env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}